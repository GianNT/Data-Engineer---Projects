{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import gzip\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, lit, sum\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType\n",
    "\n",
    "\n",
    "# Define function to catch the activities\n",
    "def log_file(message):\n",
    "    catching_time =datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(filelog, 'a') as log_writer:\n",
    "        log_writer.write(catching_time + \"---\" + message +\"\\n\")\n",
    "    return filelog\n",
    "\n",
    "# Define function to create url\n",
    "def create_url(year=2016, month=1, date=1, time=1):\n",
    "    year = f'{int(year):04}'\n",
    "    month = f'{int(month):02}'\n",
    "    date = f'{int(date):02}'\n",
    "    time = f'{int(time):02}'\n",
    "    url = f'https://dumps.wikimedia.org/other/pageviews/{year}/{year}-{month}/pageviews-{year}{month}{date}-{time}0000.gz'\n",
    "    return url\n",
    "\n",
    "\n",
    "# Define function to download file from url\n",
    "def download_gzfiles(gzfolder_path,url,downloaded_files_lock, downloaded_files):\n",
    "   \n",
    "    gzfile_name = url.split(\"/\")[-1]\n",
    "    gzfile_path = os.path.join(gzfolder_path, gzfile_name)\n",
    "\n",
    "    # Lock to ensure thread-safety when modifying downloaded_files set\n",
    "    with downloaded_files_lock:\n",
    "        # Check if the file already exists in the set\n",
    "        if gzfile_path in downloaded_files:\n",
    "            print(f\"File {gzfile_path} already exists. Skipping download.\")\n",
    "            return  # Skip the download if the file is in the set\n",
    "    \n",
    "     # Check if the file already exists\n",
    "    if os.path.exists(gzfile_path):\n",
    "        print(f\"File {gzfile_path} already exists. Skipping download.\")\n",
    "        return  # Skip the download if the file exists\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        print(f'{gzfile_name} is downloading ...')\n",
    "        subprocess.run([\"curl\", \"-o\", gzfile_path, url], check=True)\n",
    "        with downloaded_files_lock:\n",
    "            downloaded_files.add(gzfile_path)  # Mark the file as downloaded\n",
    "        print(f\"Download and decompression complete: {gzfile_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        log_file(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# Define function to run down load urls in parallel threads\n",
    "def execute_download_gzfiles(res_url, gzfolder_path,batch_size=12):\n",
    "    downloaded_files = set()  # Set to track downloaded files\n",
    "    downloaded_files_lock = threading.Lock()  # Lock to make the downloaded_files set thread-safe\n",
    "    \n",
    "    # Run in parallel\n",
    "    for idx in range(0, len(res_url), batch_size):\n",
    "        batch = res_url[idx: idx+batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            executor.map(lambda url: download_gzfiles(gzfolder_path, url, downloaded_files_lock, downloaded_files), batch)\n",
    "\n",
    "# Define function to check the .gz file downloaded not fail\n",
    "def is_valid_gz(gzfile_path):\n",
    "    try:\n",
    "        with gzip.open(gzfile_path, 'rb') as gz_check:\n",
    "            gz_check.read(1)  # Try reading one byte\n",
    "        return True\n",
    "    except (gzip.BadGzipFile, OSError):\n",
    "        return False\n",
    "         \n",
    "# Define function to check the .gz files downloaded not fail and enough files for 1 days (24 files correspond to 24 hours)\n",
    "def checkvalid_gzfile(gzfolder_path):\n",
    "    \"\"\"Ensure the folder contains 24 valid .gz files, otherwise re-download.\"\"\"\n",
    "    # Get the list of all .gz files in the folder\n",
    "    gzfile_path_list = [os.path.join(gzfolder_path, gzfile_name) \n",
    "                        for gzfile_name in os.listdir(gzfolder_path) if gzfile_name.endswith('.gz')]\n",
    "    \n",
    "    # Check if the folder contains exactly 24 files\n",
    "    if len(gzfile_path_list) == 24:\n",
    "        # Check for invalid .gz files\n",
    "        invalid_files = [gzfile for gzfile in gzfile_path_list if not is_valid_gz(gzfile)]\n",
    "        \n",
    "        if invalid_files:\n",
    "            # Remove invalid files\n",
    "            for invalid_file in invalid_files:\n",
    "                print(f\"Removing invalid file: {invalid_file}\")\n",
    "                os.remove(invalid_file) \n",
    "            # Re-download the batch\n",
    "            print(\"Re-downloading missing or invalid files...\")\n",
    "            execute_download_gzfiles(gzfolder_path)  # Adjust this function to handle the folder re-download\n",
    "    else:\n",
    "        print(f\"Expected 24 files, found {len(gzfile_path_list)}. Re-downloading...\")\n",
    "        execute_download_gzfiles(gzfolder_path)\n",
    "\n",
    "# Define function to read .gz file, transform to dataframe, and save as parquet file\n",
    "def process_single_file(spark,gzfile_name, gzfolder_path, parquet_folder_path, pagenames):\n",
    "    try:\n",
    "        gzfile_path = os.path.join(gzfolder_path, gzfile_name)\n",
    "        # Validate gzfile_name format\n",
    "        if len(gzfile_name.strip().replace(\".gz\", \"\").split('-')) != 3:\n",
    "            raise ValueError(f\"Unexpected gzfile_name format: {gzfile_name}\")\n",
    "        _, date_str, time_str = gzfile_name.strip().replace(\".gz\", \"\").split('-')\n",
    "\n",
    "        # Calculate date and time\n",
    "        date_collection = (datetime.strptime(date_str, \"%Y%m%d\") - timedelta(days=1)).strftime(\"%Y-%m-%d\")\\\n",
    "                            if time_str == \"000000\"\\\n",
    "                            else datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "        time_collection = f'{time_str[:2]}:{time_str[2:4]}:{time_str[4:]}'\n",
    "\n",
    "        output_parquetfile_path= os.path.join(parquet_folder_path, gzfile_name.replace(\".gz\", \".parquet\"))\n",
    "\n",
    "        # Skip processing if the Parquet file already exists\n",
    "        if os.path.exists(output_parquetfile_path):\n",
    "            print(f\"Skipping {gzfile_name}: Parquet file already exists.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Processing {gzfile_name}\")\n",
    "\n",
    "        # Read the .gz file into a DataFrame\n",
    "        df = spark.read.text(gzfile_path)\n",
    "\n",
    "        # Transform the DataFrame\n",
    "        df_split = df.withColumn(\"split_value\", split(df[\"value\"], \" \"))\n",
    "        df_transformed = df_split.select(\n",
    "            df_split[\"split_value\"].getItem(0).alias(\"domain\"),\n",
    "            df_split[\"split_value\"].getItem(1).alias(\"pagename\"),\n",
    "            df_split[\"split_value\"].getItem(2).cast(IntegerType()).alias(\"pageview\"),\n",
    "            df_split[\"split_value\"].getItem(3).cast(IntegerType()).alias(\"other\")\n",
    "        ).drop(\"other\").filter(col(\"pagename\").isin(pagenames))\n",
    "\n",
    "        df_final = df_transformed.groupBy(\"domain\", \"pagename\") \\\n",
    "            .agg(sum(col(\"pageview\")).cast(IntegerType()).alias(\"sumpageviewcount\")) \\\n",
    "            .withColumn(\"date_collection\", lit(date_collection).cast(StringType()))\\\n",
    "            .withColumn(\"time_collection\", lit(time_collection).cast(StringType())) \\\n",
    "            .orderBy(\"domain\", \"pagename\")\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(parquet_folder_path, exist_ok=True)\n",
    "        # Write to Parquet on the driver node\n",
    "        df_final.write.parquet(output_parquetfile_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing {gzfile_path}: {e}\"\n",
    "        print(error_message)\n",
    "        log_file(error_message)\n",
    "      \n",
    "# Define function to \n",
    "def remove_gzfiles(gzfolder_path):\n",
    "    for gzfile in os.listdir(gzfolder_path):\n",
    "        gzfile_path = os.path.join(gzfolder_path, gzfile)\n",
    "        os.remove(gzfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main logic for testing\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"gz_file_to_parquet\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    gzfolder_path = \"your_path/gzfolder\"\n",
    "    parquet_folder_path=\"your_path/parquet/2016-02\"\n",
    "    pagenames = [\"Google\",\"Facebook\",\"Amazon\",\"Microsoft\", \"Apple\", 'Walmart']\n",
    "    filelog = r\"your_path/combine_test.txt\"\n",
    "\n",
    "    for date in range(1,2):\n",
    "        # define urls \n",
    "        res_url = []\n",
    "        year_pattern = 2016\n",
    "        month_pattern = 2\n",
    "        date_pattern = date\n",
    "        time_pattern = range(0,24)\n",
    "        res_url.extend(create_url(year_pattern, month_pattern, date= date_pattern, time = i )for i in time_pattern)\n",
    "        print(res_url)\n",
    "        \n",
    "        # run tasks\n",
    "        execute_download_gzfiles(res_url)\n",
    "        checkvalid_gzfile(gzfolder_path)\n",
    "        \n",
    "        for gzfile_name in os.listdir(gzfolder_path):\n",
    "            if gzfile_name.endswith('.gz'):\n",
    "                process_single_file(spark,gzfile_name, gzfolder_path, parquet_folder_path, pagenames)\n",
    "        \n",
    "        remove_gzfiles(gzfolder_path)\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
